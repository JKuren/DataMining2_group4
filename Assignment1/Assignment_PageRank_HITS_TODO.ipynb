{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8c5a8f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "\n",
    "# Import os to utilize the built in functionality like the current working directory\n",
    "import os\n",
    "# For calculations\n",
    "import numpy as np\n",
    "# Import pandas to utilize dataframes and to read the xlsx files\n",
    "import pandas as pd\n",
    "# For pathnames\n",
    "import glob\n",
    "#For unlisting\n",
    "import itertools\n",
    "#For using math function like sqrt\n",
    "import math \n",
    "# Find out the current working directory\n",
    "#os.getcwd()\n",
    "#os.chdir('/Users/muhkas/Desktop/MK/') #use to set the path to working directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "13f18841",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess():\n",
    "    def __init__(self):\n",
    "        unique_processed_links= [] \n",
    "        \n",
    "    def loadfile(self, file_path):\n",
    "        \"\"\"Read the file and load the data\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "         file_path: str\n",
    "            A valid file path and file name contianing the data.\n",
    "            Returns\n",
    "        ------- \n",
    "        Panda Series\n",
    "            It returns Link column of the file\n",
    "\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(file_path)\n",
    "        #TODO: Load/read the files and data\n",
    "        return(df['Link'])\n",
    "\n",
    "    \n",
    "    def removeselflinks(self, file_path):\n",
    "        \"\"\"Remove the self links and extracts only the outlinks.\n",
    "           The links are preprocessed to short name eg. https://www.uu.se/contact \n",
    "           will be converted to uu.se. \n",
    "           A web page can outlink to another page more than once, so duplicates will\n",
    "           be removed.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "         file_path: path and name of file to be read\n",
    "            \n",
    "       \n",
    "        \"\"\"\n",
    "        #TODO: Call tje laodfile and store retrun values in a variable raw_data\n",
    "        raw_data = Preprocess.loadfile(self, file_path)\n",
    "        unique_raw_data = []  \n",
    "        for i in raw_data:\n",
    "            if i.find('wikipedia.org')== -1:  #Check if a link is selflink: Files were generated from\n",
    "                                          #wikipedia, therefore a link contianing 'wikipedia.org'\n",
    "                                          #represents the inlink and is removed.\n",
    "                if i.find('/', 8)!=1:  # Check if outlink has long (e-g:https://www.uu.se/contact ) \n",
    "                                   # or short (https://www.uu.se) format\n",
    "                    intermediate_name=i[0: i.find('/', 8)]\n",
    "                else:\n",
    "                    intermediate_name=i\n",
    "             \n",
    "                if intermediate_name.find('https://')==0:\n",
    "                    intermediate_name = intermediate_name[8:]\n",
    "                elif intermediate_name.find('http://')==0:\n",
    "                    intermediate_name = intermediate_name[7:]\n",
    "                else:\n",
    "                    print('error')\n",
    "        \n",
    "                if intermediate_name.find('www.')==0:\n",
    "                    intermediate_name = intermediate_name[4:]\n",
    "                \n",
    "                if intermediate_name != [] or intermediate_name!=None: \n",
    "                    unique_raw_data.append(intermediate_name)\n",
    "\n",
    "        unique_raw_data = list(dict.fromkeys(unique_raw_data))\n",
    "        self.unique_processed_links = unique_raw_data\n",
    "                #TODO:Remove http:// or https:// etc. and store result in the in variable intermediate_name \n",
    "                \n",
    "                #TODO: #Some addresses are without www. To, keep the same format, www is removed\n",
    "                #      and store result in the variable intermediate_name\n",
    "                \n",
    "                #TODO: #Remove the empty link, if any. \n",
    "                #      and append the result in the variable unique_raw_data already defined above for loop.\n",
    "                \n",
    "                 \n",
    "                #TODO: Remove duplicates from variable unique_raw_data and update self.unique_processed_links\n",
    "                \n",
    "                \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a9da355a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PopulateDictionaries(Preprocess):\n",
    "        def __init__(self):\n",
    "            self.pages={} # Create a dictionary of pages\n",
    "            self.pageindex=0 #To keep track of index of the page\n",
    "            self.inlink_dict={} #a dictionary that record in_links with pagename, pageindex, mainpageindex (page where link originated), and update mainpage indeces, if it was inlinked from more than one pages \n",
    "            self.outlink_dict={} #a dictionary that record out_links with pagename, pageindex, outlinkindex (page where link is directed to), and update mainpage indeces, if it was out bound from more than one pages \n",
    "            Preprocess.__init__(self)\n",
    "        \n",
    "        def addpages(self, list_pages):\n",
    "            \n",
    "            \"\"\"Add pages to a global dictionary of pages and index them \n",
    "       \n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "             list_pages: list\n",
    "                A processed list of all pages in a data file\n",
    "        \n",
    "            \"\"\"\n",
    "            #TODO: Add unique pages and their index in the dictionary pages\n",
    "            for i,page in enumerate(list_pages):\n",
    "                if page in self.pages:\n",
    "                    continue\n",
    "                else:    \n",
    "                    self.pages[page] = [len(self.pages)]\n",
    "\n",
    "        def inlinkgraph(self, out_links):\n",
    "    \n",
    "            \"\"\"Creates dictionary of inlink graph that records in_links with pagename, pageinde and mainpageindex,\n",
    "                If a webpage is inlinked from more than one main pages then indeces are updated.\n",
    "                For example: Consider a entery in link_dict is  'usnews.com': [[10], [1], [44]], \n",
    "                Then, webpage usnews.com has index 10 and inlinked by main pages 1 and 44.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "             out_links: list\n",
    "                A processed list of all pages in a data file\n",
    "        \n",
    "          \n",
    "            \"\"\"\n",
    "            for ind, pname in enumerate(out_links):\n",
    "                #Check if a page already exists in link_dict\n",
    "                if self.inlink_dict.get(pname)==None: # If a page is not present is dict\n",
    "                    self.inlink_dict[pname]= [ self.pages[pname], self.pages[out_links[0]] ] # Add the page and indeces\n",
    "                else: #A page already exists in the inlink_dict, update the main page indeces\n",
    "                    self.inlink_dict[pname]= [  list(itertools.chain(*self.inlink_dict[pname])), self.pages[out_links[0]]]\n",
    "            \n",
    "\n",
    "        def outlinkgraph(self, out_links):\n",
    "            \"\"\"Creates dictionary of out link graph that records out_links with pagename, pageindex and mainpageindex,\n",
    "                If a webpage has many out linkes then indeces are updated.\n",
    "                For example: Consider a entery in outlink_dict is  'abc.com': [[2], [5], [6]], \n",
    "                Then, webpage abc.com has index 2 and outlinked to  pages 5 and 6.\n",
    "\n",
    "                Parameters\n",
    "                ----------\n",
    "                 out_links: list\n",
    "                    A processed list of all pages in a data file\n",
    "                \n",
    "            \"\"\"\n",
    "            for ind, pname in enumerate(out_links):\n",
    "                #TODO: Create outlink dictionary by populating self.outlink_dict see the function description:\n",
    "                #For example: Consider a entery in outlink_dict is  'abc.com': [[2], [5], [6]], \n",
    "                #Then, webpage abc.com has index 2 and outlinked to  pages 5 and 6.\n",
    "                pindex = self.pages[pname]\n",
    "                if ind == 0:\n",
    "                    self.outlink_dict[pname] = [pindex]\n",
    "                    cur_pagename = pname\n",
    "                else:\n",
    "                    self.outlink_dict[cur_pagename].append(pindex)\n",
    "            #print(f'Current pagename: {cur_pagename}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "bc6dd076",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import link\n",
    "\n",
    "\n",
    "class AdjacencyMatrices(PopulateDictionaries):\n",
    "    def __init__(self):\n",
    "        self.adj_m_pagerank=None #Initialise the adjacancey matrix for pagerank algo.\n",
    "        self.adj_m_HITS=None    #Initialise the adjacancey matrix for HITS algo.\n",
    "        PopulateDictionaries.__init__(self) \n",
    "    \n",
    "###############Create Adjacency matrix Page rank\n",
    "    def adjpagerank(self, dict_inlinks):\n",
    "        \"\"\"Adjacacy matrix for page rank algo:  \n",
    "                        #An Adjacency matrix of in links of web pages divided by total number of out links of a page.\n",
    "                        #Each element of A that is A_i,j  represents the out link from web page 'i' (row) to web page 'j' (column).\n",
    "                        #Alternatively,  We can also say that in link from web page 'j' (column) to web page 'i' (row).\n",
    "                        #Note that  for all 'i' sum(i, M_i,j) = 1 and A must be a square matrix.\n",
    "          \n",
    "        Parameters\n",
    "        ----------\n",
    "        dict_inlinks : dictionary\n",
    "               A dictionary of in links\n",
    "        \"\"\"\n",
    "        zero_data = np.zeros(shape=(len(dict_inlinks),len(dict_inlinks)))\n",
    "        self.adj_m_pagerank = pd.DataFrame(zero_data)\n",
    "        for i in dict_inlinks:\n",
    "            link_map=(list(itertools.chain(*dict_inlinks[i])))\n",
    "            for ind, j in enumerate(link_map):\n",
    "                if (ind!=0): # It is the page index, but we need both the page index and main page index.\n",
    "                    #If a page index and main page index is similar then it is self link and is removed.\n",
    "                    if link_map[ind]!=link_map[0]:\n",
    "                        self.adj_m_pagerank.iat[link_map[ind], (link_map[0])]=1\n",
    "        ###########divide the 1 by the total out links (# Only divide if row sum is not 0)\n",
    "        self.adj_m_pagerank=self.adj_m_pagerank.apply(lambda x : x.div(x.sum()) if (x.sum()!=0) else 0 , axis=1) \n",
    "        \n",
    "        #print(len(self.adj_m_pagerank.axes[0]))\n",
    "        #print(len(self.adj_m_pagerank.axes[1]))\n",
    "\n",
    "###############Create Adjacency matrix FOR HITS\n",
    "    def adjHITS(self, dict_outlinks):\n",
    "        \"\"\"Adjacacy matrix for HITS algo:  \n",
    "                        #An Adjacency matrix of out links of web pages.\n",
    "                        #Each element of L that is L_i,j  represents the out link from web page 'i' (row) to web page 'j' (column).\n",
    "                        #Note L must be a square matrix.\n",
    "                        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dict_inlinks : dictionary\n",
    "               A dictionary of out links  \n",
    "        \"\"\"\n",
    "        zero_data = np.zeros(shape=(len(dict_outlinks),len(dict_outlinks)))\n",
    "        self.adj_m_HITS = pd.DataFrame(zero_data)\n",
    "             #TODO:Populate self.adj_m_HITS as per instructions in the assignment lecture and slides \n",
    "        for i in dict_outlinks:\n",
    "            link_map=(list(itertools.chain(*dict_outlinks[i])))\n",
    "            for ind, j in enumerate(link_map):\n",
    "                if ind!=0: # It is the page index, but we need both the page index and main page index.\n",
    "                    #If a page index and main page index is similar then it is self link and is removed.\n",
    "                    if link_map[ind]!=link_map[0]:\n",
    "                        self.adj_m_HITS.iat[ link_map[ind], (link_map[0]) ]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e97ecf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PagerankAlgo():\n",
    "    def __init__(self, A, d):\n",
    "        self.d= d       #Teleporting parameter\n",
    "        \n",
    "        self.A= A       #An Adjacency matrix of in links of web pages divided by total number of out links of a page.\n",
    "                        #Each element of A that is A_i,j  represents the out link from web page 'i' (row) to web page 'j' (column).\n",
    "                        #Note that  for all 'i' sum(i, M_i,j) = 1 and A must be a square matrix.\n",
    "        self.P= np.ones(len(self.A)) #Intial page rank =1\n",
    "\n",
    "    def calc_pagerank(self, max_itrs):\n",
    "        \"\"\"PageRank Algorithm:  This algorithm was propsed by the Larry Page and Sergey Brin at Stanford University \n",
    "                            and it ranks the web pages by measuring their importance.\n",
    "                            It is used by the search engine Google.\n",
    "                            \n",
    "        Parameters\n",
    "        ----------\n",
    "        max_itrs : int\n",
    "               Max number of iterations\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        numpy array\n",
    "            A vector of ranks such that p_i is the i-th rank in the range of [0, 1].\n",
    "    \n",
    "        Note\n",
    "        -----\n",
    "            1) Don't forget to normalize the page rank values in each iteration by max of page rank value.\n",
    "               This is done to restrict the page rank values in the range of 1-0.\n",
    "            2) Finally, normalize the page ranks by the sum of values of page ranks. This is only done at the \n",
    "               final calculation.\n",
    "               This is done to so that sum of final page ranks =1.\n",
    "        \"\"\"\n",
    "        #Check if A is square matrix\n",
    "        assert(self.A.shape[0]==self.A.shape[1])\n",
    "        #TODO: Implement PageRank algorithm according to assignment lecture and slides.\n",
    "        page_ranks = np.zeros(len(self.A))\n",
    "        n = len(self.A)\n",
    "        self.A = (1-self.d)/n + self.d * self.A\n",
    "        for i in range(0,max_itrs):\n",
    "            A_t = self.A.transpose()\n",
    "            y = A_t @ self.P\n",
    "            self.P = y/max(y)\n",
    "            \n",
    "        page_ranks = self.P/sum(self.P)\n",
    "        \n",
    "        return(page_ranks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "03e264ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HITSalgo():\n",
    "    def __init__(self, L):\n",
    "        self.L= L   #An Adjacency matrix of out links of web pages \n",
    "                    #Each element of L that is L_i,j  represents the out link from web page 'i' to web page 'j'.\n",
    "                    #Note that L must be a square matrix.\n",
    "                    \n",
    "        self.a= np.ones(len(L)) #Initial authority values =1\n",
    "        self.h= np.ones(len(L)) #Initial hub values =1\n",
    "\n",
    "\n",
    "\n",
    "    def calc_HITS(self, max_itrs):\n",
    "        \"\"\"HITS Algorithm:  HITS algorithm was propsed by Jon M. Lleinberg  at Cornell University \n",
    "                            and it ranks the web pages by measuring their authoraty and hubs.\n",
    "                            It is used by the search engine Ask.\n",
    "        Parameters\n",
    "        ----------\n",
    "        max_itrs : int\n",
    "               Max number of iterations\n",
    "    \n",
    "   \n",
    "        Returns\n",
    "        -------\n",
    "        Panda series\n",
    "            A series conisting of normalized authority and  hub scores.\n",
    "    \n",
    "        Note\n",
    "        -----\n",
    "        1) Don't forget to normalize the authority score by the sum of sequare values of all authority score. \n",
    "        2) Don't forget to normalize the hub score by the sum of sequare values of all hub score.\n",
    "        \"\"\"\n",
    "       \n",
    "        #Check if adjacency matrix is a square matrix\n",
    "        assert(self.L.shape[0]==self.L.shape[1])\n",
    "        \n",
    "        a_cal=self.a\n",
    "        h_cal=self.h\n",
    "        \n",
    "        #TODO: Implement HITS algorithm according to assignment lecture and slides.\n",
    "        for i in range(max_itrs):\n",
    "            a_cal = self.L@self.L.transpose()@a_cal\n",
    "            a_cal = a_cal/np.sqrt(sum(np.square(a_cal)))\n",
    "\n",
    "            h_cal = self.L.transpose()@self.L@h_cal\n",
    "            h_cal = h_cal/(np.sqrt(sum(np.square(h_cal))))\n",
    "\n",
    "        return(a_cal, h_cal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "335c7eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#MAIN of the code, #if __name__ == '__main__'\n",
    "pp_data = AdjacencyMatrices()\n",
    "file_list =  glob.glob('Data_files' + \"/*.csv\")\n",
    "#file_list = ['Data_files/acton.org.csv,', \"Data_files/uu.se.csv\"]\n",
    "for fl in file_list:\n",
    "    pp_data.removeselflinks(fl)\n",
    "    list_out_links= list(pp_data.unique_processed_links)\n",
    "    pp_data.addpages(list_out_links)\n",
    "    pp_data.inlinkgraph(list_out_links)\n",
    "    pp_data.outlinkgraph(list_out_links)\n",
    "    pp_data.pageindex += 1\n",
    "\n",
    "#print(f'Pages: {pp_data.pages}')\n",
    "#print('In Links: \\n', pp_data.inlink_dict)\n",
    "#print('Out Links:\\n', pp_data.outlink_dict ) \n",
    "print(' \\n')\n",
    "\n",
    "#AdjacencyMatrices\n",
    "##.  PageRank\n",
    "pp_data.adjpagerank(pp_data.inlink_dict)\n",
    "#print(\"PageRankAdjMatr \\n\", pp_data.adj_m_pagerank )\n",
    "#print( 'If the rows are summing up to one in  adj_m_pagerank:\\n',pp_data.adj_m_pagerank.sum(axis=1)[0:25])\n",
    "#print( 'Number of in-links in adj_m_pagerank: \\n',np.count_nonzero(pp_data.adj_m_pagerank, axis=0)[0:25])\n",
    "#print( 'Number of out-links in adj_m_pagerank: \\n',np.count_nonzero(pp_data.adj_m_pagerank, axis=1)[0:25])\n",
    "\n",
    "##. HITS\n",
    "pp_data.adjHITS(pp_data.outlink_dict)\n",
    "#print(\"HITSAdjMatr \\n\", pp_data.adj_m_HITS )\n",
    "# Number of outlinks in adj_m_HITS\n",
    "#print( 'Number of outlinks in adj_m_HITS:\\n',pp_data.adj_m_HITS.sum(axis=1)[0:25])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b8e75ff2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest page rank score is: 0.028857025204197615\n",
      "Page rank scores:\n",
      "                                     PageRank\n",
      "acton.org                           0.006152\n",
      "britannica.com                      0.005700\n",
      "en.wiktionary.org                   0.008776\n",
      "kth.se                              0.019785\n",
      "mbl.is                              0.005310\n",
      "isa.org                             0.004515\n",
      "blogs.sweden.se                     0.004837\n",
      "campusgotland.uu.se                 0.009638\n",
      "uu.se                               0.028857\n",
      "viaf.org                            0.011654\n",
      "ghostarchive.org                    0.009319\n",
      "catalogue.bnf.fr                    0.008576\n",
      "usnews.com                          0.010947\n",
      "afuu.org                            0.009576\n",
      "roundranking.com                    0.008184\n",
      "aop.se                              0.005800\n",
      "aleph.nkp.cz                        0.011553\n",
      "nla.gov.au                          0.009060\n",
      "afbostader.se                       0.005107\n",
      "svt.se                              0.008135\n",
      "lunduniversity.lu.se                0.002817\n",
      "fsas.uu.se                          0.008067\n",
      "ki.se                               0.022584\n",
      "opac.vatlib.it                      0.010424\n",
      "unt.se                              0.007601\n",
      "eshet.net                           0.005914\n",
      "scb.se                              0.005170\n",
      "sverigesradio.se                    0.006290\n",
      "geohack.toolforge.org               0.008633\n",
      "arkitekturupproret.se               0.005385\n",
      "webometrics.info                    0.008603\n",
      "ep.liu.se                           0.006111\n",
      "matarikinetwork.org                 0.008174\n",
      "gu.se                               0.009651\n",
      "cwur.org                            0.007986\n",
      "thestandard.com.hk                  0.010466\n",
      "dagensmedicin.se                    0.007100\n",
      "sfv.se                              0.005758\n",
      "data.bnf.fr                         0.007426\n",
      "luis.lu.se                          0.005150\n",
      "timeshighereducation.com            0.007278\n",
      "ehl.lu.se                           0.005703\n",
      "libris.kb.se                        0.007778\n",
      "the-guild.eu                        0.014570\n",
      "mediawiki.org                       0.006552\n",
      "trove.nla.gov.au                    0.006920\n",
      "scmp.com                            0.005666\n",
      "data.rero.ch                        0.007161\n",
      "idref.fr                            0.007659\n",
      "wikidata.org                        0.011423\n",
      "commons.wikimedia.org               0.008698\n",
      "bbc.co.uk                           0.007690\n",
      "catalogo.bne.es                     0.005156\n",
      "angstrom.uu.se                      0.008081\n",
      "collections.tepapa.govt.nz          0.005578\n",
      "tekniskamuseet.se                   0.006115\n",
      "data.nlg.gr                         0.008054\n",
      "ch.lu.se                            0.006236\n",
      "runeberg.org                        0.005249\n",
      "developer.wikimedia.org             0.008519\n",
      "jstor.org                           0.005779\n",
      "rudbeck.uu.se                       0.008131\n",
      "musicbrainz.org                     0.003886\n",
      "edumaritime.net                     0.004762\n",
      "wikimediafoundation.org             0.008319\n",
      "geo.uu.se                           0.012022\n",
      "creativecommons.org                 0.008747\n",
      "jur.uu.se                           0.010688\n",
      "biomus.lu.se                        0.004818\n",
      "axelsons.com                        0.003407\n",
      "doi.org                             0.007528\n",
      "kopkatalogs.lv                      0.006485\n",
      "youtube.com                         0.008529\n",
      "tsl.uu.se                           0.009272\n",
      "bbc.com                             0.010313\n",
      "chiropractorswarwick.co.uk          0.006049\n",
      "diva-portal.org                     0.005994\n",
      "manniskohjalp.se                    0.004443\n",
      "paked.net                           0.008031\n",
      "lu.se                               0.006025\n",
      "d-nb.info                           0.008600\n",
      "genus.se                            0.004772\n",
      "linnaeus.c18.net                    0.007197\n",
      "regeringen.se                       0.005112\n",
      "isni.org                            0.007772\n",
      "circ.ahajournals.org                0.005798\n",
      "pubmed.ncbi.nlm.nih.gov             0.007495\n",
      "circare.org                         0.009937\n",
      "ergo.nu                             0.007563\n",
      "retractionwatch.com                 0.008198\n",
      "e.kth.se                            0.005565\n",
      "uli.nli.org.il                      0.009815\n",
      "upsalafaktning.se                   0.010142\n",
      "cantic.bnc.cat                      0.004282\n",
      "proton-therapy.org                  0.008441\n",
      "foundation.wikimedia.org            0.007310\n",
      "topuniversities.com                 0.009085\n",
      "journals.elsevier.com               0.005825\n",
      "nytimes.com                         0.008754\n",
      "lundagard.se                        0.003696\n",
      "archive.org                         0.003831\n",
      "worldcat.org                        0.007784\n",
      "asbmb.org.au                        0.005139\n",
      "europeanspallationsource.se         0.004513\n",
      "thelocal.se                         0.007147\n",
      "getty.edu                           0.008304\n",
      "id.loc.gov                          0.007935\n",
      "ui.adsabs.harvard.edu               0.006087\n",
      "webarchive.nationalarchives.gov.uk  0.003578\n",
      "lub.lu.se                           0.007014\n",
      "api.semanticscholar.org             0.007334\n",
      "universityworldnews.com             0.005514\n",
      "leidenranking.com                   0.009234\n",
      "mises.org                           0.004315\n",
      "books.google.com                    0.008804\n",
      "lundtan.lundaekonomerna.se          0.006398\n",
      "slu.se                              0.004649\n",
      "wolffund.org.il                     0.005859\n",
      "ec.europa.eu                        0.007640\n",
      "archive.today                       0.007421\n",
      "director.co.uk                      0.003193\n",
      "student.uu.se                       0.012280\n",
      "ef.co.nz                            0.004581\n",
      "shanghairanking.com                 0.007812\n",
      "ftp.cordis.europa.eu                0.004598\n",
      "med.lu.se                           0.003552\n",
      "sydsvenskan.se                      0.005557\n",
      "authority.bibsys.no                 0.006238\n",
      "natgeotraveller.in                  0.006289\n",
      "web.archive.org                     0.007782\n",
      "ci.nii.ac.jp                        0.006294\n",
      "internt.slu.se                      0.003268\n",
      "euroscholars.eu                     0.004379\n",
      "glamourmagazine.co.uk               0.006210\n"
     ]
    }
   ],
   "source": [
    "pr= PagerankAlgo(pp_data.adj_m_pagerank , 0.85)\n",
    "page_rank_score=pr.calc_pagerank(5)\n",
    "page_rank_score = pd.DataFrame(np.transpose([page_rank_score]), index=pp_data.pages.keys())\n",
    "page_rank_score.columns = ['PageRank']\n",
    "print('Highest page rank score is:', max(page_rank_score['PageRank']))\n",
    "print('Page rank scores:\\n', page_rank_score.to_string())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "32ca0d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest authority score: 0.2353982022153797\n",
      "Highest hub: 0.143257375709201\n",
      "                       Authority       Hub\n",
      "PageName:                                 \n",
      "acton.org               0.067594  0.040806\n",
      "britannica.com          0.055520  0.098623\n",
      "en.wiktionary.org       0.094511  0.116748\n",
      "kth.se                  0.214015  0.095736\n",
      "mbl.is                  0.067181  0.072736\n",
      "...                          ...       ...\n",
      "web.archive.org         0.094829  0.130641\n",
      "ci.nii.ac.jp            0.062484  0.077974\n",
      "internt.slu.se          0.046887  0.069002\n",
      "euroscholars.eu         0.056910  0.096785\n",
      "glamourmagazine.co.uk   0.057644  0.031922\n",
      "\n",
      "[134 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "hr= HITSalgo(pp_data.adj_m_HITS )\n",
    "HITS_scores=hr.calc_HITS(5)\n",
    "HITS_scores = np.array(HITS_scores)\n",
    "scores = pd.DataFrame(index=pp_data.pages.keys())\n",
    "scores.index.name = 'PageName:'\n",
    "scores['Authority'] = np.transpose(HITS_scores[0])\n",
    "scores['Hub'] = np.transpose(HITS_scores[1])\n",
    "\n",
    "print('Highest authority score:', max(HITS_scores[0]))\n",
    "print('Highest hub:', max( HITS_scores[1]))\n",
    "print(scores) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6261c092",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 32-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "97a8f87bbbc030fa587d84ca543acba2559f1ef87cf41c6709592ea5fd23195a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
